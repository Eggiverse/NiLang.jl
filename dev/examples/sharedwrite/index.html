<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>The shared write problem on GPU · NiLang.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>NiLang.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../why/">What and Why</a></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../besselj/">Bessel function</a></li><li><a class="toctext" href="../sparse/">Sparse matrices</a></li><li class="current"><a class="toctext" href>The shared write problem on GPU</a><ul class="internal"><li><a class="toctext" href="#The-main-program-1">The main program</a></li><li><a class="toctext" href="#Check-the-correctness-of-results-1">Check the correctness of results</a></li><li><a class="toctext" href="#Why-some-gradients-not-correct?-1">Why some gradients not correct?</a></li><li><a class="toctext" href="#This-one-works:-using-a-vector-of-α-1">This one works: using a vector of <code>α</code></a></li><li><a class="toctext" href="#This-one-has-the-shared-write-problem:-using-a-vector-of-α,-but-shared-read.-1">This one has the shared write problem: using a vector of <code>α</code>, but shared read.</a></li><li><a class="toctext" href="#Conclusion-1">Conclusion</a></li></ul></li></ul></li><li><span class="toctext">API &amp; Manual</span><ul><li><a class="toctext" href="../../instructions/">Instruction Reference</a></li><li><a class="toctext" href="../../extend/">How to extend</a></li><li><a class="toctext" href="../../api/">API Manual</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Examples</li><li><a href>The shared write problem on GPU</a></li></ul><a class="edit-page" href="https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/sharedwrite.jl"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>The shared write problem on GPU</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="The-shared-write-problem-on-GPU-1" href="#The-shared-write-problem-on-GPU-1">The shared write problem on GPU</a></h1><p>We will write a GPU version of <code>axpy!</code> function.</p><h2><a class="nav-anchor" id="The-main-program-1" href="#The-main-program-1">The main program</a></h2><pre><code class="language-">using KernelAbstractions
using NiLang, NiLang.AD
using CuArrays</code></pre><p>so far, this example requires patch: https://github.com/JuliaGPU/KernelAbstractions.jl/pull/52</p><pre><code class="language-">@i @kernel function axpy_kernel(y!, α, x)
    # invcheckoff to turn of `reversibility checker`
    # GPU can not handle errors!
    @invcheckoff begin
        i ← @index(Global)
        y![i] += x[i] * α
        i → @index(Global)
    end
end

@i function cu_axpy!(y!::AbstractVector, α, x::AbstractVector)
    @launchkernel CUDA() 256 length(y!) axpy_kernel(y!, α, x)
end

@i function loss(out, y!, α, x)
    cu_axpy!(y!, α, x)
    # Note: the following code is stupid scalar operations on CuArray,
    # They are only for testing.
    for i=1:length(y!)
        out += identity(y![i])
    end
end

y! = rand(100)
x = rand(100)
cuy! = y! |&gt; CuArray
cux = x |&gt; CuArray
α = 0.4</code></pre><h2><a class="nav-anchor" id="Check-the-correctness-of-results-1" href="#Check-the-correctness-of-results-1">Check the correctness of results</a></h2><pre><code class="language-">using Test
cu_axpy!(cuy!, α, cux)
@test cuy! ≈ y! .+ α .* x
(~cu_axpy!)(cuy!, α, cux)
@test cuy! ≈ y!</code></pre><p>Let&#39;s check the gradients</p><pre><code class="language-">lsout = 0.0
@instr Grad(loss)(Val(1), lsout, cuy!, α, cux)</code></pre><p>you will see a correct vector <code>[0.4, 0.4, 0.4 ...]</code></p><pre><code class="language-">grad.(cux)</code></pre><p>you will see <code>0.0</code>.</p><pre><code class="language-">grad(α)</code></pre><h2><a class="nav-anchor" id="Why-some-gradients-not-correct?-1" href="#Why-some-gradients-not-correct?-1">Why some gradients not correct?</a></h2><p>In the above example, <code>α</code> is a scalar, whereas a scalar is not allowed to change in a CUDA kernel. What if we change <code>α</code> to a CuArray?</p><h2><a class="nav-anchor" id="This-one-works:-using-a-vector-of-α-1" href="#This-one-works:-using-a-vector-of-α-1">This one works: using a vector of <code>α</code></a></h2><pre><code class="language-">@i @kernel function axpy_kernel(y!, α, x)
    @invcheckoff begin
        i ← @index(Global)
        y![i] += x[i] * α[i]
        i → @index(Global)
    end
end

cuy! = y! |&gt; CuArray
cux = x |&gt; CuArray
cuβ = repeat([0.4], 100) |&gt; CuArray
lsout = 0.0
@instr Grad(loss)(Val(1), lsout, cuy!, cuβ, cux)</code></pre><p>You will see correct answer</p><pre><code class="language-">grad.(cuβ)</code></pre><h2><a class="nav-anchor" id="This-one-has-the-shared-write-problem:-using-a-vector-of-α,-but-shared-read.-1" href="#This-one-has-the-shared-write-problem:-using-a-vector-of-α,-but-shared-read.-1">This one has the shared write problem: using a vector of <code>α</code>, but shared read.</a></h2><pre><code class="language-">@i @kernel function axpy_kernel(y!, α, x)
    @invcheckoff begin
        i ← @index(Global)
        y![i] += x[i] * α[i]
        i → @index(Global)
    end
end

cuy! = y! |&gt; CuArray
cux = x |&gt; CuArray
cuβ = repeat([0.4], 100) |&gt; CuArray
lsout = 0.0
cuβ = [0.4] |&gt; CuArray</code></pre><p>Run the following will give you a happy error</p><blockquote><p>ERROR: a exception was thrown during kernel execution.        Run Julia on debug level 2 for device stack traces.</p></blockquote><pre><code class="language-julia">@instr Grad(loss)(Val(1), lsout, cuy!, cuβ, cux)</code></pre><p>Because, shared write is not allowed. We need someone clever enough to solve this problem for us.</p><h2><a class="nav-anchor" id="Conclusion-1" href="#Conclusion-1">Conclusion</a></h2><ul><li>Shared scalar: the gradient of a scalar will not be updated.</li><li>Expanded vector: works properly, but costs more memory.</li><li>Shared 1-element vector: error on shared write.</li></ul><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="../sparse/"><span class="direction">Previous</span><span class="title">Sparse matrices</span></a><a class="next" href="../../instructions/"><span class="direction">Next</span><span class="title">Instruction Reference</span></a></footer></article></body></html>
